{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Deep Learning with Keras\n",
    "\n",
    "This Jupyter notebook contains code and explanations for the 2018 AIS Intro to Deep Learning workshop. This tutorial focuses on training neural networks to recognize handwritten digits using the MNIST dataset.\n",
    "\n",
    "## How to Use This Notebook\n",
    "This notebook has several cells, some with markdown and others with runnable Python code. To run a cell, click on the cell and then use the **SHIFT + ENTER** keyboard shortcut or navigate to **Cell** in the top menu bar and click on **Run Cells** in the dropdown menu.\n",
    "\n",
    "## Software Prerequisites\n",
    "Make sure to install the following software/libraries:\n",
    "\n",
    "- **Anaconda** - Python distribution with many useful libraries\n",
    "- **TensorFlow** - deep learning library, acts as a backend for Keras\n",
    "- **Keras** - a high-level deep learning library that runs on top of TensorFlow\n",
    "\n",
    "## Libraries Used\n",
    "\n",
    "- **Numpy** - for handling linear algebra and numerical computations in machine learning.\n",
    "- **Matplotlib** - for visualizing data, such as images.\n",
    "- **Keras** - features a simple API for deep learning.\n",
    "\n",
    "## About the MNIST Dataset - Recognizing Handwritten Digits\n",
    "The goal of this exercise is to train a neural network to recognize handwritten digits using the famous MNIST dataset. The MNIST dataset contains a training set with 60,000 labeled 28 x 28 pixel images of the 10 digits and a test set with 10,000 images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Numpy and Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Data\n",
    "The MNIST dataset is so popular that we can download it using Keras!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "//anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above downloaded the following four parts of the dataset:\n",
    "- **X_train** - the training images, which consist of 60,000 grayscale images represented as matrices with values corresponding to the darkness of each pixel.\n",
    "- **y_train** - the training labels, which consist of 60,000 digits ranging from 0 to 9.\n",
    "- **X_test** - the test images, which consist of 10,000 grayscale images represented as matrices.\n",
    "- **y_test** - the test labels, which consist of 10,000 digits ranging from 0 to 9.\n",
    "\n",
    "Let's take a look at the shapes of these four parts of our data, which are all represented as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28)\n",
      "y_train shape: (60000,)\n",
      "X_test shape: (10000, 28, 28)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape: {}'.format(X_train.shape))\n",
    "print('y_train shape: {}'.format(y_train.shape))\n",
    "print('X_test shape: {}'.format(X_test.shape))\n",
    "print('y_test shape: {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training image set is basically a 3D stack of 60,000 28 x 28 pixel images and similarly, our test image set is a stack of 10,000 images of the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  51, 159, 253, 159,  50,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  48, 238, 252, 252, 252, 237,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         54, 227, 253, 252, 239, 233, 252,  57,   6,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  10,  60,\n",
       "        224, 252, 253, 252, 202,  84, 252, 253, 122,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 163, 252,\n",
       "        252, 252, 253, 252, 252,  96, 189, 253, 167,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  51, 238, 253,\n",
       "        253, 190, 114, 253, 228,  47,  79, 255, 168,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  48, 238, 252, 252,\n",
       "        179,  12,  75, 121,  21,   0,   0, 253, 243,  50,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  38, 165, 253, 233, 208,\n",
       "         84,   0,   0,   0,   0,   0,   0, 253, 252, 165,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   7, 178, 252, 240,  71,  19,\n",
       "         28,   0,   0,   0,   0,   0,   0, 253, 252, 195,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  57, 252, 252,  63,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0, 253, 252, 195,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0, 198, 253, 190,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0, 255, 253, 196,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  76, 246, 252, 112,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0, 253, 252, 148,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  85, 252, 230,  25,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   7, 135, 253, 186,  12,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  85, 252, 223,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   7, 131, 252, 225,  71,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  85, 252, 145,   0,   0,   0,   0,\n",
       "          0,   0,   0,  48, 165, 252, 173,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  86, 253, 225,   0,   0,   0,   0,\n",
       "          0,   0, 114, 238, 253, 162,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  85, 252, 249, 146,  48,  29,  85,\n",
       "        178, 225, 253, 223, 167,  56,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  85, 252, 252, 252, 229, 215, 252,\n",
       "        252, 252, 196, 130,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  28, 199, 252, 252, 253, 252, 252,\n",
       "        233, 145,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  25, 128, 252, 253, 252, 141,\n",
       "         37,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we see in the output above is a matrix with a lot of zeros, but we can use **matplotlib** to see what this matrix really looks like as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1226c2550>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+5Ke6Lp0ZxEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122644da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))  # the cmap argument allows us to view this as a grayscale image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the visualization above we as humans, can clearly see that this is the number 0. We can take a look at the label as well to check that the image is labeled correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for another image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x125f34c88>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12551b8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[5], cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Deep Learning? What are Neural Networks?\n",
    "Deep learning is a subfield of machine learning focused on using biologically-inspired models known as **neural networks** to solve a wide range of machine learning problems. At a high-level, neural networks are basically **mathematical models** that are based roughly on neurological concepts in **human learning**. \n",
    "\n",
    "## The Three Key Components of Neural Networks\n",
    "Neural networks can be a slightly challenging concept to grasp since they involve a mix of ideas from math, computer science, and even neuroscience. There is a lot of technical information in this workshop, so I would recommend focusing on gaining a high-level understanding of **three fundamental components** of neural networks:\n",
    "\n",
    "1. **Structure** - what the neural network looks like, including all the mathematical functions involved, the number of inputs and outputs, and the parameters, called **weights** that the network has to learn.\n",
    "    \n",
    "2. **Loss Function** - a metric that tells us how good or bad the network's predictions are. \n",
    "3. **Optimizer** - the algorithm used for **learning the weights** that give the network the best predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of Neural Networks - A Biology and Math Lesson\n",
    "As we mentioned before neural networks are **biologically inspired** models. For a moment, let's forget about machine learning and review how the human nervous system works to understand where the concept of neural networks came from. A **neuron**, the fundamental unit of this system, looks something like this:\n",
    "\n",
    "<img src='./images/neuron.png', width=400px, height=200px>\n",
    "(image source: http://home.agh.edu.pl/~vlsi/AI/intro/)\n",
    "\n",
    "### Key Parts of the Neuron\n",
    "A neuron transmits electrical signals that are constantly activated as the human brain learns and recognizes new concepts. The main parts of a neuron that we should take note of are:\n",
    "\n",
    "- **inputs** - the neuron receives several input signals through **dendrites** from connections to neighboring neurons. \n",
    "- **synapses** - gaps between **axons** and **dendrites**. Synapses transfer signals between neurons.\n",
    "- **body** - the neuron has a cell body that receives all of the incoming signals.\n",
    "- **axon** - a long connection that transmits an electrical signal **output**.\n",
    "- **activation threshold** - neurons fire when electrical activity exceeds a certain threshold.\n",
    "\n",
    "\n",
    "### The Simplest Neural Network - The Perceptron\n",
    "The perceptron, arguably the simplest neural network, was invented by psychologist Frank Rosenblatt in 1957 and looks something like this:\n",
    "\n",
    "<img src='./images/perceptron.png', width=400px, height=200px>\n",
    "(image source: http://ataspinar.com/2016/12/22/the-perceptron/)\n",
    "\n",
    "A perceptron is basically a neural network with a single **artificial neuron**. Similar to the biological neuron, a perceptron has the following characteristics:\n",
    "\n",
    "- **inputs** - the perceptron receives a given number of real-valued inputs (the inputs are numbers).\n",
    "- **weights** - the perceptron has a weight $ w_i $ associated with each input $ x_i $. These weighted connections are like synapses and they are parameters that the perceptron must \"learn\".\n",
    "- **weighted sum** - the inputs are multiplied by the weights and the results are added together to produce a weighted sum.\n",
    "- **activation function** - the perceptron has an activation function called the unit-step function that produces an output of 1 if the weighted sum is greater than some threshold $\\theta$ and -1 otherwise.\n",
    "\n",
    "### The Math Behind the Perceptron Output\n",
    "Given a vector of inputs $x = (x_0, x_1, ..., x_n) $, a set of weights $w = (w_0, w_1, ..., w_n) $, and an activation threshold $\\theta$ the output of a perceptron is given by the following function:\n",
    "\n",
    "$ f(x) = \\begin{cases} 1 & \\text{if $ wx \\geq \\theta$} \\\\ 0 & \\text{otherwise} \\end{cases} $\n",
    "\n",
    "### Example with Real Numbers\n",
    "<img src='./images/perceptron_example.png', width=500px, height=250px>\n",
    "\n",
    "### What kind of problems can the Perceptron solve?\n",
    "The Perceptron is designed to solve **binary classification problems**. By learning the best weights for a given problem, the Perceptron can be used to classify a set of inputs into one of two possible outputs. However, the Perceptron is very limited in scope because not only is it limited to solving binary classification problems, but it can only find the optimal weights for **linearly separable** binary classification problems. These are problems where a straight line or multi-dimensional hyperplane can simply separate the two classes.\n",
    "\n",
    "\n",
    "<img src='./images/perceptron_limitation.png', width=500px, height=250px>\n",
    "\n",
    "\n",
    "(image source: http://qingkaikong.blogspot.com/2016/11/machine-learning-5-artificial-neural.html)\n",
    "\n",
    "### Deep Neural Networks - Densely Connected Hidden Layers\n",
    "Deep neural networks are much more complex than the simple perceptron and feature multiple layers of neurons and several **densely connected hidden layers** between the input and output layers. Here is what a deep neural network looks like:\n",
    "\n",
    "<img src='./images/deep_net.png', width=500px, height=250px>\n",
    "\n",
    "Looking at the example above we can see that the neural network has an **input layer** with **three inputs**, two **hidden layers**, and an **output layer** with a **single output**.\n",
    "\n",
    "#### What is a densely connected layer?\n",
    "In a densely connected layer, each neuron from the group of neurons on the left is connected to every other neuron from the group of neurons on the right. In the example above, **hidden layer 1** is densely connected because every neuron from the **input layer** is connected to every neuron in **hidden layer 1**. Each connection between the layers has a numerical weight attached to it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Structure of a Deep Neural Network in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //anaconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1210: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 784)               615440    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 800)               628000    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                8010      \n",
      "=================================================================\n",
      "Total params: 1,251,450\n",
      "Trainable params: 1,251,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28)))  # Flattens the 28 x 28 image into a vector of 784 pixel values\n",
    "model.add(Dense(784, input_dim=784, activation='sigmoid'))\n",
    "model.add(Dense(800, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Loss Function and Optimizer to the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //anaconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:2745: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From //anaconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1299: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Our Data for the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 30s - loss: 0.3843 - acc: 0.8824 - val_loss: 0.4205 - val_acc: 0.8625\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 30s - loss: 0.2813 - acc: 0.9125 - val_loss: 0.2373 - val_acc: 0.9251\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 30s - loss: 0.2281 - acc: 0.9279 - val_loss: 0.2251 - val_acc: 0.9345\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 30s - loss: 0.2055 - acc: 0.9354 - val_loss: 0.2739 - val_acc: 0.9131\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 29s - loss: 0.1930 - acc: 0.9394 - val_loss: 0.2039 - val_acc: 0.9352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x126964358>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_32 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 2,174,442\n",
      "Trainable params: 2,174,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 84s - loss: 0.2414 - acc: 0.9282 - val_loss: 0.0414 - val_acc: 0.9874\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 84s - loss: 0.0456 - acc: 0.9862 - val_loss: 0.0327 - val_acc: 0.9896\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 86s - loss: 0.0325 - acc: 0.9900 - val_loss: 0.0288 - val_acc: 0.9904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12ca98c88>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "model = Sequential()\n",
    "model.add(Conv2D(kernel_size=(3, 3), filters=32, activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(kernel_size=(3, 3), filters=32, activation='relu'))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "\n",
    "model.add(Conv2D(kernel_size=(3, 3), filters=64, activation='relu'))\n",
    "model.add(Conv2D(kernel_size=(3, 3), filters=64, activation='relu'))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024, activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=256)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
